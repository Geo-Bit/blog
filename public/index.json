
[{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/categories/blog/","section":"Categories","summary":"","title":"Blog","type":"categories"},{"content":" Intro # The Horsetooth Liquidators project is something I put together as part of my involvement with NoCo Hackers, a local security meetup. We ran our CTF event for the second time, and with a bigger crowd expected, I thought it was a good chance to try building a custom CTF platform.\nProjects like Juice Shop, DVMA, Gruyere, CRAPI, etc. have always been a great way to learn about security, and I wanted to dive deeper into what it takes to develop something similar. Having benefited from CTFs myself, I wanted to give back by creating a platform that provides challenges, but is also fun, immersive, and thematic. The idea was to see what I could create over a couple of months, using the skills I\u0026rsquo;ve picked up from developing web applications over the past few years. I was also curious about how Gen AI tools like ChatGPT, Claude, and Cursor could assist in building larger projects.\nWe designed the event around a fictional Fort Collins business that needed a security check-up. Participants took on the role of security engineers, assessing the business\u0026rsquo;s security and submitting flags in the CTF. It was all about crafting a unique experience for everyone involved. Checkout the original prompt via the GitHub project.\nOh, and in case you were wondering, Horsetooth is in reference to the Horsetooth Rock (and Horsetooth Reservoir) here in Fort Collins, CO: The Vision # At the end of the day, my goal was to create a fun and immersive web experience for CTF players, packed with a variety of challenges and flags. I wanted it to feel like a real business, Horsetooth Liquidators, and make it as thematic as possible. The first year we had a small turnout, so I focused on making the platform approachable and enjoyable for all skill levels. The challenges needed to be balanced - not too hard, but not too easy either. It was all about making sure everyone had a good time while learning something new.\nPrevious Nextsads Technical Architecture # Tech Stack # I chose Vue 3 for the frontend, Express and Node for the backend, and PostgreSQL for the database. I primarily chose this stack since I had previous experience with it and it easily met the project requirements. Hosting # I went with Google Cloud for hosting as an opportunity to get more familiar with the GCP native services since most of my past experience has been with both Azure and AWS, but not so much GCP. For the project, I leveraged GCP Run Services, Google Cloud SQL, Google Cloud Storage, Google Cloud Functions, and Google Cloud Secret Manager. Docker was used for containerization to ensure consistent environments across development and production. This is what the final architecture looked like: Security Considerations and Requirements: # I didn\u0026rsquo;t want to limit the tooling people could use for the CTF, so I had to design the infrastructure to be able to handle brute forcing tools like dirbuster and hydra as well as other tools like sqlmap and Burpsuite. Fortunately, we did not encounter any performance degradation or disruptions/outages, and the infrastructure scaled and held up well overall. Securing the flags from players was a challenge, which meant that I needed to prioritize adequate authentication/authorization between the frontend and backend. Flag Development \u0026amp; Philosophy # Narrative \u0026amp; Challenge Progression # I really wanted to craft a compelling narrative to maximize player engagement and enjoyment. Players assumed the role of a security engineer hired to assist a business, progressing through challenges that mirrored real-world scenarios. Using CTFd\u0026rsquo;s challenge dependencies, my goal was to create a logical progression, but this limited the number of available challenges, affecting player motivation which I\u0026rsquo;ll touch more later.\nSomething for Everyone # I aimed to create CTF challenges that would appeal to a wide range of skill levels. We had participants for whom this was their first CTF event, as well as veteran CTF players. My goal was to cater to everyone if possible. Ultimately, I had to make a decision to preioritize one or the other, which I\u0026rsquo;ll touch on later in the Lessons Learned section.\nSomething New # I wanted to create challenges that were relevant to modern security practices, but also straightforward to solve. My hope was to include some modern CVE examples and dabble with AI/LLM vulnerabilities to provide something fresh to the player. Though since this CTF included brand new players with minimal web security experience, it was a good opportunity to include some of the more basic OWASP Top 10 type challenges.\nChallenges # Ultimately, I ended up developing 8 web challenges:\nBackup Protocol (25pts) - CWE-312 Admin Access Requires (75pts) - CWE-347 Inventory Overflow (50pts) - CWE-20 Rookie Mistake (25pts) - CWE-312 Order Up! (50pts) - CWE-639 Confused Fox (25pts) - CWE-74 Late Night Easter Egg (25pts) - CWE-312 Support Ticket Snooping (100pts) - CWE-79 You can find the full descriptions of challenges on the GitHub project at horsetooth-liqudators/CHALLENGES.md.\nHow It Went # A few thoughts on what went well and what could have gone better\u0026hellip;\nBuilding the Thing # Tech Stack \u0026amp; Infra # Overall, everything was \u0026#x1f44d;, but a few things worth highlighting:\nI had to pivot from originally having everything deployable with Docker Compose to building and serving individual images to Google Cloud Run. I also had to move away from deploying Postgres via Docker to using Google Cloud SQL, which was fine but required a bit more setup in GCP. Virtual Machines are no fun. For a moment, I was going to pivot to using IaaS/VMs so I could retain my Docker Compose setup, but I quickly realized how much of a pain reployment was going to be, and I\u0026rsquo;m glad I went serverless. Deployment # I never fully automated the deployments to Google Cloud, and I would have liked to. Fortunately I didn\u0026rsquo;t have to deal with any major outages or bug fixes, so I wasn\u0026rsquo;t ever overwhelmed with the re-deployment process. The GCP CLI tool made redeploying a breeze, so once I was able to test in dev using Docker Compose, I just ran the gcloud builds like:\n# DEPLOY BACKEND gcloud builds submit --config cloudbuild.yaml \\ --substitutions=_INSTANCE_CONNECTION_NAME=\u0026#34;GCP_SQL_INSTANCE_NAME\u0026#34;,\\ _DB_USER=\u0026#34;postgres\u0026#34;,\\ _DB_PASSWORD=\u0026#39;SUPER_SECRET_PASSWORD\u0026#39;,\\ _DB_NAME=\u0026#34;horsetooth_liquidators\u0026#34;,\\ _FRONTEND_URL=\u0026#34;FRONTEND_URL\u0026#34;,\\ _JWT_SECRET=\u0026#34;$(openssl rand -base64 32)\u0026#34; # DEPLOY FRONTEND gcloud builds submit --config cloudbuild.frontend.yaml More Data # I would have liked to invest more into telemetry and observability data. I was very curious about how people were interacting with and \u0026ldquo;poking\u0026rdquo; at the application. Google Cloud actually offers decent analytics, telemetry, and observability services (ie. Google Cloud Monitoring, Google Cloud Logging, and Google Cloud Trace). I would have liked to use these services to achieve better visibility and understanding of user interactions and behaviors during the CTF event but unfortunately this feel to the wayside.\nCTF Design Insights # Flag Validation and Security # I initially overlooked securing the flags from players. Midway through development, I moved all flag delivery to the backend, making them inaccessible from the frontend. Browser features like the developer console can easily detect hard-coded flags. The Chatbot had a bug with the drop-down prompts, which broke while I focused on the freeform text option. I had to push a hotfix to fix the issue.\nChallenge Progression and Difficulty Curves # Looking at the correlation of # of points to % solved, I feel like I did a decent job balancing the challenges with the exception of Iventory Overflow, which no one solved unfortunately and was supposed to be more of a \u0026ldquo;Medium\u0026rdquo; difficulty challenge. Unfortunately, this is the result of having a very specific expected solution method in mind, and not being flexible enough to allow for other strategies. I assumed users would leverage something like Burpsuite to enumerate the inventory endpoint and brute force inventory values. This proved to be more elusive than I had hoped, and led to players ultimately missing out on the challenge.\nTesting and Validation (oops) # Honestly, I completely dropped the ball on testing, which fortunately only led to 1-2 bugs. One challenge in particular involved cracking a password hash, and I was confident the hash could easily be cracked using a tool like Hashcat or John, but I was wrong. I ended up having to providing players with a custom password file to use, which significantly simplified the challenge and made it less fun. This could have be avoided if I would have tested the hash myself, but unfortunately I was short on time. In the future, I will definitely write unit tests for all of the challenges which actually would also be a fun challenge in itself.\nNext Time - More Modular! # Assuming I\u0026rsquo;m developing something brand new and not just forking this project, I want to take a more modular approach to the project and challenges. I would like develop a \u0026ldquo;shell\u0026rdquo; of an ecommerce web application, for example, and make each challenge a potentially added feature of the platform, but not have them be dependencies or related to one another. Ideally, I could develop something that I could continue to recycle and add on to each year.\nAI Tooling - not perfect, but pretty damn good # This was by far the largest project I have developed using AI tooling. This was my first time using Cursor to help develop the majority of features, specifically using Claude 3.5 Sonnet. These AI models still have trouble keeping up with large code bases, and I ran into several instances where the model would lose context and start deviating from the original prompt, so in the later stages of development, it could be a headache getting the model back on track and re-feeding it sufficient context to continue progressing. For specific re-occuring patterns and features, it would be super helpful to be able to define a \u0026ldquo;recipe\u0026rdquo; for the model to follow. Things as simple as the preferred flag generation pattern:\npython -c \u0026#39;from secrets import token_hex; print(f\u0026#34;noco{{{token_hex(16)}}}\u0026#34;)\u0026#39; I noticed that because of the contextual limitations, it really requires the developer to know be able to identify deviations and be familiar enough with web development (or general software development) to get the AI back on track. While the fixes typically involve clever prompting, I did have to revert to previous commits at times.\nWhen it came to generating flavor text for the web application and challenges, AI was extremely helpful. By feeding it the initial Horsetooth Liquidators story prompt, I had it build me the About Page, the Hiring Page, the Inbox messages, and the challenge descriptions with incredible efficiency. This really helped speed up development and is what primarily allowed me to keep the challenges and narrative on-theme and engaging.\nPlayer Experience # We went from 3 users to over 15 users this year, which makes me tremendously proud and excited for future events. To me, the amount of participation and feedback reinforced the value of the event and the opportunity to learn and grow.\nI had several participants reach out to me with feedback and appreciation for the web app and challenges, which was super cool. I had a few instances of unexpected solutions and approahces which was also fun to see. One player accidentally completed the Support Ticket Snooping challenge while trying to solve the Inventory Overflow challenges which is awesome. Support Ticket Snooping was a XSS challenge tht involved devising a payload that would expose a admin-panel iframe to the user. The CTF player was using fuzzing tools that helped them identify the solution while also trying to brute force our inventory endpoint to find the flag.\nIt\u0026rsquo;s also worth noting that we leveraged our community Discord to host comms for the event. This live communication stream actually helped pull more players into the event, and gave players the opportunity to work with the development team, report issues, and more. We had a private channel for discussing challenge development as well, which helped with coordination.\nConclusion # While I feel like I could have prioritized more time on designing the challenges themselves, I learned a lot about the value of hands-on security education. Unfortunately, because I spent so much time developing the platform, I didn\u0026rsquo;t get to spend any time actually playing the CTF. But I\u0026rsquo;m a big fan of the \u0026ldquo;learn by doing\u0026rdquo; approach, and I think this CTF event was a great opportunity to do just that, so I\u0026rsquo;m glad I was able to learn something myself, andprovide a fun and engaging experience for everyone involved.\nYou can do it too! Seriously, AI assisted dev makes developing these types of projects accessible. It\u0026rsquo;s given me the confidence to build ambitious projects that I would have otherwise been challenging, especially in terms of time and resources. Especially for building fun and interactive experiences. A great example of this is the mock terminal 404 page I built, which was a simple prompt and took less than an hour to build.\nResources # Checkout the official GitHub project at GitHub Project ","date":"30 December 2024","externalUrl":null,"permalink":"/posts/horsetooth-ctf/","section":"Posts","summary":"A deep dive into developing a deliberately vulnerable web application for a local security meetup CTF event, exploring the challenges, learning opportunities, and the balance between education and entertainment.","title":"Building a CTF Web Application: Lessons from Horsetooth Liquidators","type":"posts"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/cloud-development/","section":"Tags","summary":"","title":"Cloud Development","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/ctf/","section":"Tags","summary":"","title":"CTF","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/","section":"Geo","summary":"","title":"Geo","type":"page"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/node.js/","section":"Tags","summary":"","title":"Node.js","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/vue.js/","section":"Tags","summary":"","title":"Vue.js","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/web-security/","section":"Tags","summary":"","title":"Web Security","type":"tags"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/tags/appsec/","section":"Tags","summary":"","title":"AppSec","type":"tags"},{"content":" Securing the Ship: Tackling Docker Image Flaws # I\u0026rsquo;ve spent some time delving into the complexities of finding and remediating vulnerabilities in Docker container images over the past several months. This article explores various detection mechanisms, including references to specific tooling, and provides remediation processes for addressing vulnerabilities in Docker container images. I\u0026rsquo;ll discuss the differences between software composition analysis (SCA) vulnerabilities and first-party vulnerabilities, and the challenges associated with remediation efforts. Additionally, I\u0026rsquo;ll share personal observations and insights from my experience in the field.\nDetection Mechanisms # Software Composition Analysis (SCA) Tools # SCA tools are designed to analyze the components within your Docker images and identify known vulnerabilities. Some popular SCA tools include:\nSnyk: Snyk integrates with Docker to scan images for known vulnerabilities and provides actionable remediation advice. I found Snyk particularly useful during a recent project where we needed to ensure the security of our containerized applications without disrupting the development workflow. Aqua Security: Aqua Security offers comprehensive scanning for vulnerabilities in Docker images and integrates seamlessly with CI/CD pipelines. In my experience, Aqua Security\u0026rsquo;s deep integration with various DevOps tools makes it a go-to solution for continuous monitoring. Clair: Clair is an open-source project that scans Docker images for vulnerabilities in the packages installed in them. Clair’s flexibility and ease of integration into custom CI/CD pipelines were invaluable in a recent security assessment I conducted. # Example: Scanning a Docker image with Snyk snyk container test my-docker-image First-Party Vulnerabilities # First-party vulnerabilities refer to issues introduced by the custom code and configurations within your Docker images. Detecting these requires a combination of static analysis, dynamic analysis, and manual review.\nStatic Code Analysis: Tools like SonarQube and Bandit can analyze your source code for security issues before it\u0026rsquo;s built into a Docker image. During one code audit, using SonarQube revealed a critical flaw in our custom middleware, which could have led to a significant data breach if not addressed. Dynamic Analysis: Tools like OWASP ZAP can be used to perform dynamic testing against running containers to identify vulnerabilities. I\u0026rsquo;ve seen dynamic analysis catch runtime issues that static analysis tools missed, highlighting the importance of a multi-faceted approach. # Example: Using Bandit for static code analysis bandit -r /path/to/your/code Personal Observations # During my time threat hunting in Docker environments, I noticed several patterns and common issues. One frequent observation was the over-reliance on outdated base images, which often contained numerous vulnerabilities. In a collaborative work environment, I found that regular security training for developers significantly reduced the number of first-party vulnerabilities introduced into our Docker images.\nA lot of the popular IaC scanning tools recommend adding additional lines to the Dockerfile to update individual dependencies. While this approach works, I feel that it is not as efficient as it could be. These tools often lack the context and consideration of all dependencies being used, and common transitive dependencies within multiple components. For example, if you have 5 transitive dependencies in your Dockerfile, you could update explicity update those depedencies, or simply update the transitive top-level dependency which could resolve all the vulnerable transitives.\nRemediation Processes # Updating Base Images # One of the most straightforward remediation steps is to update the base images used in your Dockerfiles. Ensure you\u0026rsquo;re using the latest, secure versions of base images.\n# Example: Updating a Dockerfile to use a secure base image FROM ubuntu:20.04 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ nodejs \\ npm COPY . /app RUN cd /app \u0026amp;\u0026amp; npm install CMD [\u0026#34;node\u0026#34;, \u0026#34;/app/index.js\u0026#34;] Patching Dependencies # Ensuring that all dependencies are up to date is crucial. Tools like Dependabot can automate dependency updates, making it easier to keep your Docker images secure.\n# Example: Using Dependabot to automate dependency updates version: 2 updates: - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;daily\u0026#34; Implementing Security Best Practices # Adopt best practices for Docker image security, such as minimizing the attack surface by using minimal base images like Alpine, and ensuring proper user permissions within containers. This was a game-changer in a project where we had to harden our containers for a financial services client.\n# Example: Using a minimal base image FROM alpine:3.12 RUN apk add --no-cache nodejs npm COPY . /app RUN cd /app \u0026amp;\u0026amp; npm install USER node CMD [\u0026#34;node\u0026#34;, \u0026#34;/app/index.js\u0026#34;] Challenges in Docker Vulnerability Remediation # Remediating vulnerabilities in Docker images is not without its challenges. Some key difficulties include:\nDependency Hell: Managing and updating dependencies can be complex, especially when dealing with large, multi-layered Docker images. I remember a particular instance where conflicting dependencies between two essential services caused significant delays in our deployment schedule. Continuous Monitoring: Docker images need continuous monitoring for new vulnerabilities, requiring integration with CI/CD pipelines and constant vigilance. At my workplace, setting up automated scans and integrating them with our CI/CD pipeline helped catch vulnerabilities early in the development cycle. Balancing Security and Functionality: Ensuring security without breaking functionality can be a delicate balance, often requiring in-depth testing and validation. This balance is critical, as I’ve seen how overly restrictive security measures can stifle productivity and lead to workarounds that introduce new risks. Key Takeaways # Regular Scanning: Regularly scan your Docker images using both SCA tools and first-party vulnerability detection methods. This proactive approach has saved me from countless potential incidents. Automate Updates: Automate dependency and base image updates to reduce the risk of vulnerabilities. Automation tools have significantly reduced the manual overhead and error rate in our processes. Follow Best Practices: Implement Docker security best practices to minimize the attack surface and enhance overall security. Best practices, when consistently followed, create a robust baseline security posture. Continuous Vigilance: Continuously monitor and update Docker images to stay ahead of emerging threats. Staying updated with the latest security trends and vulnerabilities is part of my daily routine. Recent research from the application security industry highlights the critical importance of proactive vulnerability management in Docker environments. Studies by organizations like Snyk and Aqua Security emphasize that regular scanning and prompt remediation are essential to maintaining secure containerized applications. The dynamic nature of threats makes continuous learning and adaptation crucial.\n","date":"1 July 2024","externalUrl":null,"permalink":"/posts/2024-07-01-securing-the-ship/","section":"Posts","summary":"","title":"Securing the Ship: Tackling Docker Image Flaws","type":"posts"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/tags/vulnerability-management/","section":"Tags","summary":"","title":"Vulnerability Management","type":"tags"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":" Azure Subdomain Takeover \u0026amp; Dangling DNS # As a security researcher and application security engineer, I frequently encounter the issue of dangling DNS or subdomain takeovers, particularly within Microsoft Azure environments. This occurs when web applications are deleted, but their associated CNAME records on custom domains are not removed. This leaves the domain vulnerable to malicious actors who can redirect traffic to their own infrastructure.\nUnderstanding the Problem # When you create a web application in Azure and assign it a custom domain using a CNAME record, you establish a link between your domain and Azure\u0026rsquo;s resources. If these resources are later deleted without removing the CNAME record, the domain remains pointed to a now non-existent resource. This creates a \u0026ldquo;dangling\u0026rdquo; DNS entry.\nWhat Happens Next? # Domain Availability Check: Attackers can periodically check for available subdomains. Resource Re-creation: They can then create a resource in Azure with the same name, effectively taking over the subdomain. Malicious Redirection: The hijacked subdomain can be used to serve malicious content, phish for credentials, or spread malware. Real-world Example # Let\u0026rsquo;s say you have a CNAME record pointing app.example.com to myapp.azurewebsites.net. If myapp is deleted but the CNAME record remains, an attacker can claim myapp in Azure, and app.example.com will now point to their resource.\nDNS Provider Configuration # For demonstration, we\u0026rsquo;ll use a common DNS provider, such as GoDaddy, to illustrate how to check and remove these records.\nexample.com ├── www.example.com (CNAME -\u0026gt; myapp.azurewebsites.net) ├── app.example.com (CNAME -\u0026gt; myapp.azurewebsites.net) └── blog.example.com (A -\u0026gt; 192.0.2.1) Mitigation and Remediation Strategies # Regular Audits # Perform regular audits of your DNS records to ensure they are still valid. Tools and scripts can automate this process.\n# Simple script to check DNS records for domain in $(cat domains.txt); do nslookup $domain | grep \u0026#34;can\u0026#39;t find\u0026#34; \u0026amp;\u0026amp; echo \u0026#34;$domain is dangling\u0026#34; done Automated Cleanup # Implement automated cleanup of DNS records when deleting Azure resources. Azure Resource Manager (ARM) templates and scripts can be configured to remove DNS entries.\n{ \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/dnszones/CNAME\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-05-01\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;TTL\u0026#34;: 3600, \u0026#34;CNAMERecord\u0026#34;: { \u0026#34;cname\u0026#34;: \u0026#34;myapp.azurewebsites.net\u0026#34; } }, \u0026#34;dependsOn\u0026#34;: [\u0026#34;[resourceId(\u0026#39;Microsoft.Web/sites\u0026#39;, \u0026#39;myapp\u0026#39;)]\u0026#34;] } ] } Use Azure Managed Services # Where possible, use Azure-managed services that handle DNS records for you. Azure Front Door, for example, can manage your custom domains and reduce the risk of dangling DNS.\nConclusion # Dangling DNS records pose a significant security risk, especially in dynamic cloud environments like Azure. By understanding the problem and implementing the strategies outlined above, you can mitigate and remediate these risks effectively. Regular audits, automated cleanups, and utilizing managed services are key steps to securing your domains.\nStay vigilant and keep your infrastructure secure!\n","date":"20 June 2024","externalUrl":null,"permalink":"/posts/2024-06-20-azure-subdomain-takeover/","section":"Posts","summary":"","title":"Azure Subdomain Takeover \u0026 Dangling DNS","type":"posts"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS","type":"tags"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"29 September 2023","externalUrl":null,"permalink":"/tags/alerting/","section":"Tags","summary":"","title":"Alerting","type":"tags"},{"content":" Introduction # Proxmox Virtual Environment (Proxmox VE) is a powerful open-source virtualization platform that combines two virtualization technologies: KVM (Kernel-based Virtual Machine) for virtual machines and LXC (Linux Containers) for lightweight container-based virtualization. Monitoring your Proxmox environment is crucial for ensuring its reliability and performance. In this guide, we\u0026rsquo;ll explore how to set up automated Proxmox monitoring and alerting to keep your virtual infrastructure in check.\nPrerequisites # Before we begin, make sure you have the following prerequisites:\nA Proxmox Virtual Environment (Proxmox VE) installation. Access to the Proxmox web interface. A server or virtual machine running an operating system that supports Docker. Step 1: Install Docker and Docker Compose # Proxmox monitoring and alerting can be simplified by using Docker containers. Start by installing Docker and Docker Compose on a server or VM that can communicate with your Proxmox host. You can follow the official Docker installation instructions for your chosen operating system.\nStep 2: Create a Docker Compose Configuration # Create a directory for your Proxmox monitoring configuration files. Inside this directory, create a docker-compose.yml file with the following content:\nversion: \u0026#34;3\u0026#34; services: prometheus: image: prom/prometheus container_name: prometheus ports: - 9090:9090 volumes: - /path/to/prometheus-config:/etc/prometheus command: - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; alertmanager: image: prom/alertmanager container_name: alertmanager ports: - 9093:9093 volumes: - /path/to/alertmanager-config:/etc/alertmanager command: - \u0026#34;--config.file=/etc/alertmanager/config.yml\u0026#34; Replace /path/to/prometheus-config and /path/to/alertmanager-config with the paths to your configuration directories.\nStep 3: Configure Prometheus # Create a prometheus.yml file inside your /path/to/prometheus-config directory with the following content:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#34;proxmox\u0026#34; static_configs: - targets: [\u0026#34;proxmox-host-ip:9100\u0026#34;] Replace \u0026lsquo;proxmox-host-ip\u0026rsquo; with the IP address or hostname of your Proxmox host.\nStep 4: Configure AlertManager # Create a config.yml file inside your /path/to/alertmanager-config directory. Here\u0026rsquo;s a sample configuration for AlertManager:\nroute: group_by: [\u0026#34;alertname\u0026#34;] group_wait: 30s group_interval: 5m repeat_interval: 1h receiver: \u0026#34;default\u0026#34; receivers: - name: \u0026#34;default\u0026#34; email_configs: - to: \u0026#34;your-email@example.com\u0026#34; Configure the email recipient address in the to field.\nStep 5: Start Docker Containers # Navigate to the directory containing your docker-compose.yml file and run the following command to start Prometheus and AlertManager in Docker containers:\ndocker-compose up -d Step 6: Configure Proxmox Alerts # In the Proxmox web interface, navigate to \u0026ldquo;Datacenter\u0026rdquo; \u0026gt; \u0026ldquo;Nodes\u0026rdquo; \u0026gt; \u0026ldquo;Your Node\u0026rdquo; \u0026gt; \u0026ldquo;System\u0026rdquo; \u0026gt; \u0026ldquo;Email.\u0026rdquo; Set up email notifications, and configure alerts to send emails to your AlertManager.\nConclusion # Automated Proxmox monitoring and alerting using Docker containers, Prometheus, and AlertManager ensures that your Proxmox virtual environment stays healthy and responsive. You can receive timely alerts about system health and take proactive measures to maintain a reliable virtual infrastructure. Monitoring and alerting are essential components of managing your Proxmox VE effectively, ensuring your virtual machines and containers run smoothly.\n","date":"29 September 2023","externalUrl":null,"permalink":"/posts/2023-09-29-proxmox-monitoring/","section":"Posts","summary":"","title":"Automated Monitoring and Alerting for Proxmox","type":"posts"},{"content":"","date":"29 September 2023","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"Monitoring","type":"tags"},{"content":"","date":"29 September 2023","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]